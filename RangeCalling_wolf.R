
# Date: 08th of Nov, 2023
# Files: .txt

##########################################Brief summary of the calling history###############################################################
#We experimented with several strategies to merge the peaks generated by GBS-MeDIP. Initially, we employed an arbitrary division (ADJW) of 300 bps into adjacent windows. Subsequently, we utilized MACS2 to call peaks with various types of filters, finding that the "narrow peaks" strategy yielded the best results. However, a limitation of MACS2 was that it tended to ignore several coverage peaks, leading to the collapse of multiple adjacent regions into peaks spanning 23,000 bps.
#Another approach involved in silico digestion with the restriction enzyme PstI. However, this method presented discrepancies with in vitro digestion, as certain motifs were preferentially digested, leading to inaccuracies in peak formation. Upon observing the nature of the peaks in a "merged" BAM file, we realized that any strategy to merge peaks would inevitably create 20k peaks, representing a mosaic of reads, albeit comprising only around 5% of the total number of peaks.
#To address this challenge, we concluded that merging peaks based solely on coverage peak data was insufficient. Instead, we needed the coordinates of all pairs of reads (R1 and R2) to accurately merge peaks by considering the overlap of coordinates rather than coverage peak. Sorting the coordinates of the paired-ends and defining a % overlap between them proved to be a promising approach. However, our initial attempts resulted in unexpected outcomes, with giant windows being merged first due to reads distributed throughout.
#As a revised strategy, we merged peaks based on paired-end coordinates and developed a script to divide peaks when a 50% coverage drop occurred, particularly in dense regions. To mitigate the risk of dividing R1 and R2 pairs again, we selectively divided only windows above the third quartile limit, a small percentage of regions. Despite these efforts, the initial peaks remained challenging to manage due to their unique nature.
#In a final attempt, we identified regions with coverage above a specified threshold across multiple BAM files. This involved calculating coverage, aggregating data, identifying regions with sufficient coverage, and merging overlaps. Additionally, we created a custom summary function and defined a method to split regions into subpeaks based on a specified length, which were then saved into an R data file for further analysis.
#Upon comparing this approach with the paired-end method, we observed an increase of around 15% in the number of peaks obtained, indicating its effectiveness in peak detection.
##############################################################################################################################
##############################################################################################################################
##############################################################################################################################
##############################################################################################################################
#This method is designed to create a universe for your approach and can be executed using GBS-MeDIP or GBS data. We strongly recommend using GBS bam files as input instead of GBS-MeDIP for optimal results.

# R code starts here, this codes is meant to run in bash
####################################################################################
#######################################################################################
##############checking the GBS library - GBS UNIVERSE ##############
library(Rsamtools)
library(GenomicRanges)
library(dplyr)
library(foreach)
library(doParallel)
library(rtracklayer)
library(ggplot2)
library("BSgenome.Cfamiliaris.UCSC.canFam3")
bsgenome <- BSgenome.Cfamiliaris.UCSC.canFam3
# Register a parallel backend using doParallel
# Set the number of cores you want to use
n_cores <- 2
#n_cores <- suppressWarnings(detectCores())

  # Ensures that the cluster is stopped when the script exits

######HEADER#######################
#First: You will define where are your bam files and which files you want to work on
#bam_dir <- "/yourBamFiles/Aligned"
#Example
bam_dir <- "/proj/naiss2024-23-57/GBS_MeDIP_benchmark/datasets/wolf"
bam_files <- list.files(path = bam_dir, pattern = ".bam$", full.names = TRUE)

setwd(bam_dir)
dir.create(file.path(getwd(), "merged"), showWarnings = FALSE, recursive = TRUE)
output_path <- file.path(getwd(), "merged")

# Define the minimum coverage threshold
threshold <- 1  # Modify this based on your requirement
# This will store the regions with coverage >= X for all files

all_regions <- foreach(bam_file = bam_files, .packages = c("Rsamtools", "GenomicRanges", "dplyr")) %dopar% {
  tryCatch({
    coverage_data <- pileup(bam_file)
    aggregated_coverage <- coverage_data %>%
      group_by(seqnames, pos) %>%
      summarize(seq_depth = sum(count))
    regions <- aggregated_coverage[aggregated_coverage$seq_depth >= threshold, ]
    gr <- GRanges(seqnames=regions$seqnames,
                  ranges=IRanges(start=regions$pos, end=regions$pos),
                  seq_depth=regions$seq_depth)
    return(gr)
  }, error=function(e) {
    message("Error processing file: ", bam_file, "; Error: ", e$message)
    NULL  # Return NULL if there's an error
  })
}


#save(gr, file="gr_min1.rda")
# Transform list to GRangesList for compatibility with reduce function
#all_regions <- GRangesList(all_regions)
all_regions <- compact(all_regions)

# Merge overlapping regions

#unlistedregions <- unlist(all_regions)

merged_regions <- reduce(all_regions[[1]])
for (i in 2:length(all_regions)) {
  merged_regions <- reduce(c(merged_regions, all_regions[[i]]), min.gap = 5)
}

# Assuming output_path has been correctly set as shown previously in your session
save(merged_regions, file=file.path(output_path, "merged_regions.rda"))


custom_summary <- function(gr) {
  num_ranges <- length(gr)
  avg_length <- mean(width(gr))
  min_length <- min(width(gr))
  max_length <- max(width(gr))
  quartiles <- quantile(width(gr), probs = c(0.25, 0.5, 0.75))
  
  cat("Summary of Genomic Ranges Object 'merged_regions':\n")
  cat("Number of Ranges:", num_ranges, "\n")
  cat("Average Length:", avg_length, "\n")
  cat("Minimum Length:", min_length, "\n")
  cat("Maximum Length:", max_length, "\n")
  cat("1st Quartile Length:", quartiles[1], "\n")
  cat("2nd Quartile Length (Median):", quartiles[2], "\n")
  cat("3rd Quartile Length:", quartiles[3], "\n")
}
custom_summary(merged_regions)

# Load the stringr package
library(stringr)

# Print the extracted average length value
average_length <- mean(width(merged_regions))
print(average_length)

#Summary of Genomic Ranges Object 'merged_regions':
#  Number of Ranges: 392140
#Average Length: 178.8228
#Minimum Length: 1
#Maximum Length: 3918
#1st Quartile Length: 139
#2nd Quartile Length (Median): 151
#3rd Quartile Length: 199

# Call the custom summary function with your GenomicRanges object
region_lengths <- width(merged_regions)
region_lengths_df <- data.frame(length = region_lengths)

# Create a histogram with the appropriate range
tiff(file.path(output_path, "histPeaks.tiff"), units="cm", width=50.00, height=50.00, res=600, compression = "lzw")
ggplot(region_lengths_df, aes(x = length)) +
  geom_histogram(binwidth = 10, color = "black", fill = "blue") +
  xlim(0, 500) +  # Adjust based on your data's distribution
  labs(title = "Fragment Length Histogram",
       x = "Fragment Length (bp)",
       y = "Frequency")
dev.off()

png(file.path(output_path, "histPeaks.png"))  # Use the appropriate format (e.g., pdf, png, jpg)
hist(region_lengths, breaks = 30, main = "Fragment Length Histogram",
     xlab = "Fragment Length (bp)", ylab = "Frequency")
dev.off()

# Subset regions above 300 bps
above_300_bps <- subset(merged_regions, width(merged_regions) > 300)
# Subset regions below or equal to 300 bps
below_300_bps <- subset(merged_regions, width(merged_regions) <= 300)
#test
#above_300_bps <- above_300_bps[1:4]
# Assuming 'above_300_bps' is your GenomicRanges object with regions above 300 bps
# 'average_length' is the average length calculated from 'merged_regions'


library(GenomicRanges)

# Function to split GRanges into segments of approximately average_length
split_ranges_to_average <- function(gr, average_length) {
  # Ensure average_length is a positive integer
  if (!is.numeric(average_length) || average_length <= 0) {
    stop("average_length must be a positive numeric value")
  }
  
  # Initialize an empty GRanges object for the result
  result_gr <- GRanges()
  
  # Iterate over each range in the input GRanges object
  for(i in seq_along(gr)) {
    range <- gr[i]
    seqname <- seqnames(range)
    strand <- strand(range)
    range_start <- start(range)
    range_end <- end(range)
    range_width <- width(range)
    
    # Calculate the number of segments to split the range into
    num_segments <- ceiling(range_width / average_length)
    
    # Calculate starts and ends for each segment
    segment_starts <- range_start + (0:(num_segments-1)) * average_length
    segment_ends <- pmin(segment_starts + average_length - 1, range_end)
    
    # Adjust segment_starts to ensure no overlap
    if (length(segment_starts) > 1) {
      segment_starts[-1] <- segment_ends[-length(segment_ends)] + 2
      segment_ends[-length(segment_ends)] <- segment_starts[-1] - 1
    }
    
    # Create a new GRanges object for the segments
    segments <- GRanges(seqnames = rep(seqname, num_segments),
                        ranges = IRanges(start = segment_starts, end = segment_ends),
                        strand = rep(strand, num_segments))
    
    # Combine with the result
    result_gr <- c(result_gr, segments)
  }
  
  return(result_gr)
}


# Split regions in 'above_300_bps' into subpeaks with an average length of 'average_length'
# Ensure you have defined 'above_300_bps' and 'average_length' before this step
subpeaks_above_300_bps <- split_ranges_to_average(above_300_bps, average_length)

#test with subset
#subset_above_300_bps <- above_300_bps[1:10]
#subpeaks_above_300_bps <- split_ranges_to_average(subset_above_300_bps, average_length)

# Assuming 'subpeaks_above_300_bps' is the result from the split_into_subpeaks function
# and 'below_300_bps' is another GRanges object you mentioned

# Combine the two GRanges objects
merged_regions_final <- c(subpeaks_above_300_bps, below_300_bps)

custom_summary(merged_regions_final)
average_length <- custom_summary(merged_regions_final)

region_lengths <- width(merged_regions_final)
# Create a histogram with the appropriate range
tiff(file.path(output_path, "histPeaks_final.tiff"), units="cm", width=50.00, height=50.00, res=600, compression = "lzw")
ggplot(region_lengths_df, aes(x = length)) +
  geom_histogram(binwidth = 10, color = "black", fill = "blue") +
  xlim(0, 500) +  # Adjust based on your data's distribution
  labs(title = "Fragment Length Histogram",
       x = "Fragment Length (bp)",
       y = "Frequency")
dev.off()

png(file.path(output_path, "histPeaks_final.png"))  # Use the appropriate format (e.g., pdf, png, jpg)
hist(region_lengths, breaks = 30, main = "Fragment Length Histogram",
     xlab = "Fragment Length (bp)", ylab = "Frequency")
dev.off()


final_sorted <- sortSeqlevels(merged_regions_final)
final_sorted <- final_sorted[order(seqnames(final_sorted), start(final_sorted))]

#Filtering single positions
SinglePositions <- final_sorted[start(final_sorted) >= end(final_sorted)]
# Filter out ranges where start >= end
final_sorted <- final_sorted[start(final_sorted) < end(final_sorted)]


# 5. Final statistics
avg_final <- mean(width(final_sorted))
sd_final <- sd(width(final_sorted))
print(paste("Average:", avg_final, "Standard Deviation:", sd_final))
#[1] "Average: 146.042675939584 Standard Deviation: 50.9604909104586"

output_file <- file.path(output_path, "joined_sorted.bed")
export(final_sorted, con=output_file)

save(final_sorted, file=file.path(output_path, "final_sorted.rda"))

library(GenomicRanges)

# Assuming final_sorted is your GRanges object

# Step 1: Extract necessary information
chr <- as.character(seqnames(final_sorted))
start <- start(final_sorted)
end <- end(final_sorted)
strand <- as.character(strand(final_sorted))

# Step 2: Create GeneID
gene_id <- paste0("Gene_", seq_along(chr))

# Step 3: Create a data frame in SAF format
saf_df <- data.frame(GeneID = gene_id, Chr = chr, Start = start, End = end, Strand = strand)

# Step 4: Export the data frame to a SAF file
write.table(saf_df, file=file.path(output_path, "final_sorted.saf"), quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)

